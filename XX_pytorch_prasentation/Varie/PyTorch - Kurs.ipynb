{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warum lieber PyTorch?\n",
    "\n",
    "Tesorflow:\n",
    "- von Google\n",
    "- Unglaublich Low-Level --> Fast alles selber schreiben\n",
    "    - Flexibilität aber vielleicht...\n",
    "    - Code nicht mehr so gut lesbar :-)\n",
    "    \n",
    "- Man nutzt lieber Keras  \n",
    "    - Nicht von Google\n",
    "    - Debugging nicht so geil :/\n",
    "    - Wirklich easy\n",
    "- Static computation Graph!\n",
    "\n",
    "PyTorch:\n",
    "- von Facebook\n",
    "- High Level (eine Art Keras)\n",
    "- sehr jung --> Windows Version lange \"beta\" geblieben\n",
    "- Viel Umstieg nach PyTorch\n",
    "- Dynamic computation graph :-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static computation Graphs\n",
    "\n",
    "Beispiel: Daten -> Convolution -> Fully Connected\n",
    "\n",
    "Der Graph ist statisch:\n",
    "- Das Bild ist immer genau dasselbe, z.B. 32x32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Computation Graph\n",
    "- Die Anzahl an Neuronen kann sich zur Laufzeit verändern\n",
    "    - If und else Verzweigungen sind bsp. möglich\n",
    "    - Man kann eine Berechnung für die ersten X-Epochen machen, danach sie verändern\n",
    "Daten -> Convolution:\n",
    "    - If something:\n",
    "        - Pooling A\n",
    "    - Else:\n",
    "        - Pooling B\n",
    "-> Fully Connected    \n",
    "    - Der Input-Layer passt sich quasi den Inputs :-)\n",
    "    - Wie kann das genau so schnell sein, wenn Änderungen zur Laufzeit möglich sind? Läuft fast genau so schnell wie TensorFlow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2789, -0.2773, -0.0543],\n",
       "        [-0.3424,  1.5185, -1.3986],\n",
       "        [-0.6996,  2.1158, -0.3792],\n",
       "        [ 2.5705,  0.6989, -0.7818],\n",
       "        [ 1.1512, -0.1644,  0.2251]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,3) #Matrix 5x3 (5 Zeilen, 3 Spalten)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0781,  1.7730],\n",
       "         [ 0.1685, -0.5971]],\n",
       "\n",
       "        [[-1.1920, -0.6030],\n",
       "         [ 1.1426,  3.2733]],\n",
       "\n",
       "        [[-0.6611, -0.2770],\n",
       "         [-0.5768,  1.3310]],\n",
       "\n",
       "        [[ 0.0602,  2.3778],\n",
       "         [ 1.8561,  0.3390]],\n",
       "\n",
       "        [[-0.7297,  0.2842],\n",
       "         [-0.0853,  0.6249]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr3 = torch.randn(5,2,2) #5 2x2 Matrixen\n",
    "matr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(1,10,(2,3))\n",
    "b = torch.randint(1,10,(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementenweise Operationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:  tensor([[  9.,   5.,  12.],\n",
      "        [  7.,   4.,   6.]]) \n",
      "oder: tensor([[  9.,   5.,  12.],\n",
      "        [  7.,   4.,   6.]])\n",
      "************************************************************\n",
      "Subtraktion:  tensor([[ 3.,  1., -6.],\n",
      "        [ 5.,  2.,  0.]]) \n",
      "oder: tensor([[ 3.,  1., -6.],\n",
      "        [ 5.,  2.,  0.]])\n",
      "************************************************************\n",
      "Multiplikation:  tensor([[ 18.,   6.,  27.],\n",
      "        [  6.,   3.,   9.]]) \n",
      "oder: tensor([[ 18.,   6.,  27.],\n",
      "        [  6.,   3.,   9.]])\n",
      "************************************************************\n",
      "Division:  tensor([[ 2.0000,  1.5000,  0.3333],\n",
      "        [ 6.0000,  3.0000,  1.0000]]) \n",
      "oder: tensor([[ 2.0000,  1.5000,  0.3333],\n",
      "        [ 6.0000,  3.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Addition: \", torch.add(a,b), \"\\noder:\", a+b)\n",
    "print(\"*\"*60)\n",
    "print(\"Subtraktion: \", torch.sub(a,b), \"\\noder:\", a-b)\n",
    "print(\"*\"*60)\n",
    "print(\"Multiplikation: \", torch.mul(a,b), \"\\noder:\", a*b)\n",
    "print(\"*\"*60)\n",
    "print(\"Division: \", torch.div(a,b), \"\\noder:\", a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrizenmultiplikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA: Berechnungen auf der Grafikkarte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0356,  3.0388, -1.1823,  ...,  1.0606,  1.0627, -0.2655],\n",
       "        [ 1.6800, -0.2064, -0.2593,  ...,  1.9300, -0.8253,  0.1486],\n",
       "        [-0.8029,  1.2764,  2.2142,  ...,  2.4638,  0.6943,  0.0832],\n",
       "        ...,\n",
       "        [ 1.8547, -0.6906, -1.1779,  ..., -1.1348,  3.3192, -0.1652],\n",
       "        [-2.8268, -1.0857,  3.2563,  ...,  1.2146,  0.5481, -1.6993],\n",
       "        [ 1.1480, -0.0482, -0.7900,  ..., -1.7810,  0.8314,  0.5277]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((60000,784))\n",
    "y = torch.randn((60000,784))\n",
    "#Tensors nach CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "    x = x.cuda()#\n",
    "    y = y.cuda()\n",
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenes Neuronales Netzwerk in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F #FUnktionen\n",
    "from torch.autograd import Variable #Variablen - kann man backpropagieren\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Klasse vom Neuronalen Netz erzeugen\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        #Konstruktor aus Superklasse\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        #Layer definieren\n",
    "        self.lin1 = nn.Linear(10,11)\n",
    "        self.lin2 = nn.Linear(11,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Berechnet bis zum Output \"\"\"\n",
    "        x = F.relu(self.lin1(x))\n",
    "        #Letzter Schritt ohne Aktivierungsfunktion\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\" \n",
    "        Ausgabe von den Berechnungen aus nn.Module \n",
    "        x ist eine Variable fuer einen Tensor, Tensor ist drinnen\n",
    "        \"\"\" \n",
    "        print(x.size())\n",
    "        print(x.size()[1:])\n",
    "        size = x.size()[1:] #ausser Batch-Dimension\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *=i \n",
    "        return num    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bestehendes Netz wird geladen\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "if os.path.isfile(\"net.pt\"):\n",
    "    print(\"Bestehendes Netz wird geladen\")\n",
    "    net = torch.load(\"net.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Netz lernt\n",
    "\n",
    "1. Criterion für Berechnung des Fehlers\n",
    "2. Optimizer für das Lernen\n",
    "\n",
    "Speichern mit `torch.save()`, Laden mit `torch.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [0,1,1,0,1,0,0,0,0,0] #\n",
    "target = Variable(torch.Tensor([label for _ in range(10)])) #10x5 targets\n",
    "if torch.cuda.is_available():\n",
    "        target = target.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler:  1.3628334910782171e-14\n",
      "Fehler:  1.3578027929978843e-14\n",
      "Fehler:  1.23345776341789e-14\n",
      "Fehler:  1.1979306605112029e-14\n",
      "Fehler:  1.162542301601276e-14\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    # Baut aus einem Tensor eine Variable, Tensoren sind die Inhalte davon\n",
    "    x = [1,0,0,1,0,1,1,1,1,1] #\n",
    "    input = Variable(torch.Tensor([x for _ in range(10)])) #10x5 targets\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.cuda()\n",
    "    out = net(input)\n",
    "    #print(out) #10x5\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(out, target)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Fehler: \", loss.item())\n",
    "    #print(\"Fehler in der 1.Stufe: \", loss.grad_fn.next_functions[0][0])\n",
    "    net.zero_grad() #Veraenderung auf null setzen\n",
    "    loss.backward() #Backpropagation des Loss\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.10)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net,\"net.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST mit PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Daten herunterladen und bereitstellen\n",
    "## ToTensor --> Umwandlung der rohen Daten\n",
    "## Normalisierung ist dann notwendig fuer die Verarbeitung\n",
    "## transforms.Compose fuer die Zusammenstellung der Operationen\n",
    "# Normalize: Mean, STD\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} #Nur wenn CUDA aktive \n",
    "train_data = torch.utils.data.DataLoader(\n",
    "                        datasets.MNIST('data', train=True, download=True,\n",
    "                        transform=transforms.Compose([transforms.ToTensor(), \n",
    "                        transforms.Normalize((0.1307,),(0.308,))])),\n",
    "                       batch_size = 64, shuffle=True, **kwargs)\n",
    "\n",
    "test_data = torch.utils.data.DataLoader(\n",
    "                        datasets.MNIST('data', train=False,\n",
    "                        transform=transforms.Compose([transforms.ToTensor(), \n",
    "                        transforms.Normalize((0.1307,),(0.308,))])),\n",
    "                       batch_size = 64, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.308,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.308,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netz, self).__init__()\n",
    "        #1. Schicht = CONV (Kernel 5x5)\n",
    "        #In-Channel: Wie viele Bilder rein\n",
    "        #Out-Channel: Wie viele rauskommen\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20, kernel_size=5)\n",
    "        \n",
    "        #Buch lesen --> Jedes Wort ist ein Pixel\n",
    "        #Alle Wörter danach noch im Kopf? Nein!\n",
    "        #Dropout Layer --> Ich kann nicht jedes Pixel, aber was auf dem Bild steht doch schon\n",
    "        self.conv_dropout = nn.Dropout2d() # Das Netz lernt somit nicht auswendig\n",
    "        \n",
    "        #Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(320,60)\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #1. Schicht ist Convolution\n",
    "        x = self.conv1(x)\n",
    "        #Aktivierungsfunktion\n",
    "        x = F.max_pool2d(x, 2) #Input + Kernel size\n",
    "        #Relu gut fuer Conv-Schichten\n",
    "        x = F.relu(x)\n",
    "        #Schicht 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        #x.size() ist z.B.[64,20,4,4]\n",
    "        #64 == Batch size\n",
    "        #20 sind die Unterbilder der Dim 4x4 (16 Pixel)\n",
    "        #20 Bilder, je 16 Pixel --> 20*4*4 = 320 --> Fully Connected --> Das hängt von kernel_size ab\n",
    "        \n",
    "        #Lineare Transformation\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x) #Attenzione! Da ultimo non si usa la f di attivazione\n",
    "        return F.log_softmax(x) #Softmax: Output mit größtem Wert wird auf 1 gesetzt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Netz(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv_dropout): Dropout2d(p=0.5)\n",
       "  (fc1): Linear(in_features=320, out_features=60, bias=True)\n",
       "  (fc2): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Netz()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n",
      "Durchschnitt-Loss:  tensor(2.3276, device='cuda:0')\n",
      "Accuracy:  tensor(0)\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, gpu=True):\n",
    "    model.train() # Lernen\n",
    "    #Batch_id == Anzahl an Batches\n",
    "    # data = \n",
    "    for batch_id, (data, target) in enumerate(train_data):\n",
    "        if gpu:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "        \n",
    "        #Berechnung des Loss\n",
    "        criterion = F.nll_loss #2 Losses \n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_id*len(data), \n",
    "        len(train_data.dataset), \n",
    "        100.* batch_id/len(train_data), \n",
    "        loss.data[0]))\n",
    "\n",
    "def test(gpu=True):\n",
    "    model.eval() #Gewichte \"eingefroren\"\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data: #Keine Batch_Size in Test daten\n",
    "        if gpu:\n",
    "            target = target.cuda()\n",
    "            data = data.cuda()\n",
    "            data = Variable(data, volatile=True)\n",
    "        else:\n",
    "            data = Variable(data)\n",
    "        target = Variable(target)    \n",
    "        out = model(data)\n",
    "        #size_average == ob Durchschnitt berechnet werden soll\n",
    "        loss += F.nll_loss(out, target, size_average=False).data[0] #Summe des Batch-Losses\n",
    "        prediction = out.data.max(1, keepdim=True)[1]\n",
    "        #print(out.data.size()) #[64,10]\n",
    "        correct += prediction.eq(target.data.view_as(prediction)).cpu().sum() #Uebertragen auf CPU\n",
    "        #print(correct)\n",
    "    loss = loss/len(test_data.dataset) #Durchschnit\n",
    "    acc = 100.*(correct/len(test_data.dataset))\n",
    "    print(\"Durchschnitt-Loss: \", loss)\n",
    "    print(\"Accuracy: \", acc.data)\n",
    "\n",
    "for epoch in range(1,10):\n",
    "    #train(epoch)\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
