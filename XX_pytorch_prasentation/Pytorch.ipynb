{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. pyTorch: Einführung\n",
    "    - Funtkionen\n",
    "2. pyTorch Fundamentals: Tensors\n",
    "    - pyTorch und Numpy \n",
    "3. Neuronale Netze in pyTorch\n",
    "    - Ein simples neuronale Netz\n",
    "    - MNIST Beispiel mit der Klasse nn.Module\n",
    "    - MNIST Beispiel mit der Klasse nn.Sequential\n",
    "4. Autograd: Berechnung der lokalen Ableitung\n",
    "5. CNN in pyTorch\n",
    "6. RNN in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. pyTorch: Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eine ML Open-Source-Bibliothek für python\n",
    "- Basiert auf in Lua geschriebene Bibliothek Torch\n",
    "- Von dem Facebook-Forschungsteam für K.I. entwickelt \n",
    "- Erscheinungsjahr: 2016\n",
    "- GPU sowie CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU beschleunigte Tensor-Analyse\n",
    "- Neuronale Netze auf Basis eines bandbasierten Autograd-Systems\n",
    "- NumPy, SciPy und Cython sind dabei verwendbar\n",
    "- Beim Deep Learning:\n",
    "    * Viel Flexibilität\n",
    "    * Hohe Geschwindigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pyTorch Fundamentals: Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1af78dde510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "torch.manual_seed(0) #Seed CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5410, -0.2934, -2.1788],\n",
       "        [ 0.5684, -1.0845, -1.3986]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3,device=device, dtype=dtype)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  9.,  8.],\n",
       "        [ 6.,  6.,  8.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(10, (2,3), device=device)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  3.,  6.],\n",
       "        [ 9.,  1.,  4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randint(10, (2,3), device=device) \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 24.,  27.,  48.],\n",
       "        [ 54.,   6.,  32.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b # Ausgabe einer Multiplikation der beiden Tensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1,2] # Ausgabe des Elements in der 3. Spalte der 2. Zeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available\")\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2 x 3], m2: [2 x 3] at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\th\\generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d9add567418c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Matrixenmultiplikation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [2 x 3], m2: [2 x 3] at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\th\\generic/THTensorMath.c:2033"
     ]
    }
   ],
   "source": [
    "## Matrixenmultiplikation\n",
    "torch.mm(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch bietet drei Instanzmethoden: [`reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_) und [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "resh_b = b.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilet\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py:255: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "res_b = b.resize(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_view = b.view(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  52.,  115.],\n",
       "        [  54.,  110.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(a,b_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyTorch und Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy zu Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.ones((2,2))\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_from_np = torch.from_numpy(np_array)\n",
    "torch_from_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis:** Datentypen sind hier wichtig. Nicht alle Umwandlungen sind erlaubt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.int8. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-2731267156ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp_array_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_array_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.int8. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "np_array_new = np.ones((2, 2), dtype=np.int8)\n",
    "torch.from_numpy(np_array_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **Umwandlung von NumPy zu Torch** ist nur zwischen den folgenden Datentypen erlaubt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| NumPy        | Torch            |\n",
    "| :-------------: |:--------------:|\n",
    "| int64     | LongTensor |\n",
    "| int32     | IntegerTensor |\n",
    "| uint8      | ByteTensor      |\n",
    "| float64 | DoubleTensor     |\n",
    "| float32 | FloatTensor      |\n",
    "| double | DoubleTensor      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch zu NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor = torch.randint(10,(2,2))\n",
    "type(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_to_numpy = torch_tensor.numpy()\n",
    "type(torch_to_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors und Gradienten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch bietet mit dem Parameter `requires_grad` die Möglichkeit, Gradienten zu einem bestimmten Tensor zu akkumulieren, d.h. eine Variable ist dann trainierbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((2,2), requires_grad=True)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_no_grad = torch.ones(2,2)\n",
    "b_no_grad = torch.ones(2,2)\n",
    "b_no_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randint(5,(2,2), requires_grad=True)\n",
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x0000025102AA61D0>\n"
     ]
    }
   ],
   "source": [
    "summe = a + b\n",
    "print(summe.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Summe ohne Gradienten zu berücksichtigen\n",
    "print((a_no_grad+b_no_grad).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.],\n",
      "        [ 3.,  1.]])\n",
      "<MulBackward1 object at 0x0000025102AE3278>\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "print(torch.mul(a, b).grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neuronale Netze in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein simples neuronale Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematische Bedeutung: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Inner-Product-Darstellung:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper #Klasse mit Helper-Funktionen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Attention to the shapes: \n",
    "        For a tensor a with shape (64,10) and a tensor b with shape (64,)\n",
    "        while doing a/b pyTorch will give an error --> Division is done across the columns (broadcasting)\n",
    "        b should have a shape of (64,1). pyTorch will divide all the 10 values of a by the 1 value of b.\n",
    "        dim param\n",
    "            dim = 0 --> Operation is done across the rows\n",
    "            dim = 1 --> Operation is done across the columns\n",
    "    \"\"\"\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Daten herunterladen und bereitstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: C:\\Users\\dilet/.pytorch/MNIST_data/\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader) #Createas an iterable\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af7e111668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([64, 256])\n",
      "out shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Flatten the input images\n",
    "#(64, -1)\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "hidden_units = 256\n",
    "output_units = 10\n",
    "input_units = 784\n",
    "\n",
    "# Create parameters\n",
    "###### First layers parameters\n",
    "w1 = torch.randn(input_units, hidden_units)\n",
    "b1 = torch.randn(hidden_units)\n",
    "\n",
    "##### Hidden layer parameters\n",
    "w2 = torch.randn(hidden_units, output_units)\n",
    "b2 = torch.randn(output_units)\n",
    "\n",
    "#### Results from forward pass from first layer to hidden layer\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "print(\"h shape:\", h.shape)\n",
    "\n",
    "#### Result from forward pass from hidden layer to output layer\n",
    "out = torch.mm(h, w2) + b2\n",
    "print(\"out shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0277e-07,  3.0864e-03,  3.7085e-06,  1.3403e-09,  1.4350e-08,\n",
      "          4.6569e-08,  3.6875e-08,  4.8599e-04,  2.8210e-11,  9.9642e-01],\n",
      "        [ 2.5225e-06,  9.9149e-08,  8.3739e-06,  1.6046e-04,  9.1268e-01,\n",
      "          1.4821e-06,  1.7629e-06,  9.4206e-03,  1.5321e-09,  7.7728e-02],\n",
      "        [ 6.8520e-06,  2.6222e-06,  1.4227e-06,  3.3753e-05,  4.1699e-02,\n",
      "          9.3135e-09,  6.2620e-06,  9.5163e-01,  1.8790e-10,  6.6194e-03],\n",
      "        [ 1.0832e-02,  2.3909e-06,  9.0262e-01,  9.0823e-10,  5.3198e-06,\n",
      "          7.3441e-11,  1.5513e-08,  8.1042e-03,  6.8827e-14,  7.8436e-02],\n",
      "        [ 4.4666e-07,  3.6747e-12,  4.2474e-01,  8.0220e-08,  3.2646e-04,\n",
      "          2.1494e-11,  6.9239e-05,  5.4601e-04,  7.6000e-13,  5.7432e-01]])\n",
      "torch.Size([64, 10])\n",
      "tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "probabilities = softmax(out)\n",
    "print(probabilities[:5])\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Beispiel mit der Klasse `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation        \n",
    "        self.hidden = nn.Linear(784, 256)#input and output size\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante mit `torch.nn.functional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Mit Sigmoid and Softmax\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mit Relu und Softmax\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gewichte und der Bias werden automatisch initialisiert, können trotzdem personalisiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1.00000e-02 *\n",
       "       [[-1.0726,  3.4053, -0.1737,  ...,  0.8206,  3.3972,  3.3185],\n",
       "        [-2.4427,  0.1889,  1.1872,  ..., -1.1976, -1.1817, -2.6984],\n",
       "        [-1.6340, -0.2432, -1.3721,  ..., -1.6280,  0.7623,  2.4890],\n",
       "        ...,\n",
       "        [-2.8540, -0.5980, -3.3948,  ..., -1.8858,  2.8487, -2.2695],\n",
       "        [-3.4781,  1.3896, -1.8033,  ..., -1.8471, -2.6723, -0.9228],\n",
       "        [ 1.8333,  0.2930, -0.2163,  ..., -0.8927,  2.8090,  2.3755]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1.00000e-02 *\n",
       "       [ 1.7354, -1.2313, -0.9047, -1.9414, -0.7431,  0.2301,  1.6740,\n",
       "         2.7051,  0.6252, -0.1610,  2.0617, -0.1641,  0.3622, -0.7649,\n",
       "         1.0525, -3.0952, -0.2539, -0.9378,  3.4697, -2.9377, -0.6113,\n",
       "         0.5249,  0.5240,  1.1483,  2.2860, -1.3353, -0.0996,  0.0615,\n",
       "        -3.1515,  1.0033, -2.8843,  1.7604,  3.4325,  2.5382, -0.3251,\n",
       "         0.7631, -0.7682, -0.8566,  2.0864, -2.0984,  2.8646,  1.1656,\n",
       "        -1.8832,  0.1338, -1.0488, -2.1127,  0.0880,  0.2161, -2.4469,\n",
       "         2.2210, -0.5025, -2.6257, -2.4667, -2.8893,  2.5986,  0.2493,\n",
       "         0.3597, -3.1645,  2.8668, -1.1518,  2.5710,  2.0573, -0.4425,\n",
       "        -1.8080,  2.7289,  2.8354,  0.5891, -2.4242, -1.2957,  0.2273,\n",
       "         1.5842,  2.8071,  1.8272,  0.6003, -2.6663, -2.9327, -3.0711,\n",
       "        -1.7219,  0.6364, -2.3769, -1.0427,  1.8957, -2.9660, -2.1548,\n",
       "        -1.9090,  1.4173,  0.4880,  2.8593, -0.5192,  0.4613,  2.5323,\n",
       "         2.2451,  1.8837,  0.0520, -2.5246, -0.4242,  2.9478, -2.8304,\n",
       "        -3.3281,  1.9605, -2.8106,  2.2326,  3.2937,  0.5836,  2.4410,\n",
       "         1.2984,  0.3842,  0.4745,  0.2035,  3.0229,  1.8186, -1.3436,\n",
       "         0.3563,  3.1460, -3.4000, -3.4603,  2.4555,  2.9613, -0.2076,\n",
       "         3.0138,  3.3766,  0.0339,  2.0789,  3.4983, -2.9096,  0.1571,\n",
       "         0.6496, -2.3831])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalisierung der Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias.data.fill_(0) # Daten werden über der .data.methode geändert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4558e-04, -1.0050e-02, -6.0389e-03,  ..., -1.0791e-02,\n",
       "          7.9316e-03, -1.5470e-03],\n",
       "        [ 1.6753e-02,  2.4354e-03, -3.3371e-04,  ..., -9.0441e-03,\n",
       "          1.0012e-02,  1.9656e-03],\n",
       "        [-3.3192e-03,  8.4949e-03,  8.0176e-04,  ..., -2.1354e-03,\n",
       "          5.2285e-03, -5.5238e-03],\n",
       "        ...,\n",
       "        [-1.4705e-03, -7.3541e-03, -1.6336e-02,  ...,  3.2338e-03,\n",
       "         -4.8613e-03, -6.9639e-03],\n",
       "        [ 1.4552e-02, -1.0538e-02, -1.2753e-02,  ..., -5.8222e-03,\n",
       "         -5.9688e-03, -5.4915e-03],\n",
       "        [-1.2478e-02, -1.6951e-02,  4.8201e-03,  ..., -1.5148e-02,\n",
       "          8.0123e-03, -1.9058e-02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 226,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "print(images.shape)\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps) # Das Netzwerk hat noch nichts gelernt :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Beispiel mit der Klasse `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHECAYAAAAOFHoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3XmYJWV9L/DvDxBFVgUBReOoAcGgQYiKu6gxJkTF3WtE0RgTNZq45AaXRDSa4HXDJQkxisYlcUvUG8F9RXEdJV4UxIVxwQUB2Udkee8fVS1t2z01p+d0nz5nPp/nOU/1qaq36ndqenr6O+9bb1VrLQAAACxtm0kXAAAAsNYJTgAAAAMEJwAAgAGCEwAAwADBCQAAYIDgBAAAMEBwAgAAGCA4AQAADBCcAAAABghOAAAAAwQnAACAAYITAADAAMEJAABggOAEAMyMqmr9a92ka9laTOqab8l5q+qNfdtjNve4VXVUv/4Ty6uYaSc4AQBrTlVdt6qeWFX/XVXfq6rLqurSqjqrqt5VVY+qqh0mXedqqaoN836hn3tdVVXnVdXJVfW0qrrupOvcWvWh6piqOmjStbBytpt0AQAA81XV/ZK8Nsne81ZfmuTqJOv614OTvLiqjmytfWy1a5ygS5Nc0n+9fZLrJ7lL/3p8VR3WWjtnUsVNkR8l+UaSc0doc2Hf5nuLbDsqyd2TbEhy6hbWxhqlxwkAWDOq6qgk70kXmr6R5Mgke7TWdmqt7ZJktyQPSfKJJDdKcrfJVDoxL22t7d2/rp9kjyQvStKS3Cpd4GRAa+1ZrbX9W2uvGaHNu/s2j17J2li7BCcAYE2oqtskOT7d7ycnJblta+0trbXz5vZprV3YWvvP1tphSR6e5OLJVLs2tNbOa609N8kb+lUPqKobTbImmFWCEwCwVrwoybWTnJ3kka21jZvaubX2jiQv35wDV9W2VXVYVb2yqtZX1U+q6hdV9cOqendV3XMTbbfp72H5eH9P0RVV9dOq+lpVnVBV912kzc2q6p+r6syq2tjfo/XdqvpEVT2rqvbYnLpH8B/zvj54Xh2/nAShqq5dVc+pqq9W1cX9+t0W1H1YVf1XVf24vz4/Hro+C9ofWFVv69v9vKrOqKq/qaprL7H/TlX10Kp6a1WdVlUX9NfrW1X12qrad4XOu+TkEJs4x69NDjG3Lt0wvSR5w4L70Db0+53Qv3/XwDme3+93yubWxepxjxMAMHFVtU+Sw/u3r2qtXbg57VprbTNPcUCS+fdCXZ7kF0lumOSIJEdU1XNaa3+/SNs3J3nkvPcXJtkl3TC5W/WvD8xtrKqD0w0l3LlfdUW6e5N+o3/dPclX5rcZg7Pnfb3LItuvk+RTSW7f13PZwh2q6oVJntO/bek+55655voc21p71iZquFO6oYI7JrkoSSW5ZZIXJPmDqvrd1tolC9ocleTV895fnO4/9m/Rvx5ZVUe01j4y5vOOy8YkP0l3r9m1+vPPD/w/7ZevS/LYJPerqt3n96LOqapK8pj+7QkrVC9bQI8TALAW3CPdL7xJ8n9X4Pi/SPLOJPdLd//UDq21nZLsleRvklyV5IVVdYf5jarqbulC09VJnpZkl9babumCyI3S/eL/6QXnemm60PT5JAe31rZvrV0v3S/2t0tyXLpQMk6/Me/rCxbZ/uQk+yV5RJKd+s+wLl2gS1U9IteEptck2bOv+Qa5JtgcXVWP2kQN/5Tk60lu01rbNd01eGy6IHFoFu8dPK8//p2S7Nbfx3addEH3remu2b9X1Y5jPu9YtNbe3lrbO8lcD9FfzLsHbe/W2u36/U7pa9w+yR8tcbh7Jblpuj+Tt69UzSyf4AQArAUH9MvL000KMVattTNbaw9rrb2vtfaTuZ6q1to5rbUXJnl+uuD2ZwuaHtovP9RaO661dnHfrrXWftRa+7fW2jOXaPMXrbWvzKvhstbal1prT2utfXbMH/FP5k6T5IuLbN8pycP7X/R/0dfz3dbaFX1Px9/1+72ttfaU1tq5/T7ntdaemmuGAr6wqpb6/fHyJPdtrf2/vu0vWmtvTPKkfvsfV9VN5zdorf1Ha+2prbXPzvUy9tf2jHQTg3wkXXh7yCY++8jnnZDX9cvHLrH9cf3yXXPfZ6wtghMAsBbs3i9/NsLwu3H673555wXrL+qXe24iMCw01+aGW1zVJlTV9lV1q6p6Xbrp2ZMu+Px0kd2/2lr70BKHOijJb/Zfv3CJfZ7fL2+abrjfYo5vrZ2/yPo3JflBut87H7hE21/Tfx+c2L9d+OeyYuddQW9K1/N5UFXddv6Gqto119RomN4aJTgBAFuFqtqhf1DsJ6rqnH6Sh9bf3D/XM7RwRrqPpPtl9+Akn6juwbtDs9ad1C/fVFXHVtWhVXWtMX2M582r+fIkX0vyx/22z+WaXpaFNtXDNTeZxE9ba19bbIfW2jdyzX1UBy+2T7r7uhZre3WSk5dqW1U3rqoX95N2XFDdg33nPuMr+t02dc2Xdd7V1t/X9J7+7cJep0emG6L4zdbap1a1MDab4AQArAVzN8tfrx86NlZVdcN0DyZ9ebrJGW6QLnj8NN3N/XMPQv2Ve2laa99K8sR098vcNd1EEWdX1Vn9rHm/0nPQ+6t097zsnOSv04WWi6rqY1X1xKraYQs+yqV9vT9J8sMkpyf5r3TD2u7aWlvs/qbkmkkKFnODfnn2JvZJut6b+fsvtKn2c9t+pW1V3T3dZ/jf6cLNrukmiJj7jHO9d5u6x2nk807Q3HC9R1bV9vPWzw3Te0NYswQnAGAtOL1fXjvdjGjjdly6yRG+k25Y2/X7h+ru2d/cf+hSDVtrJyS5WZK/TPLedCFvXbr7odZX1bMX7H9ekrsk+d0kr0rXm7V9ksPSTWRwWlXdeJmfY/4DcPdprd2qtfbg/nlXV26i3VWbcexFp+4ek18Lw30v3FvS3X/1kXQPM96htbbb3GdM8vSl2i/3vBP2kSRnpRuaev8kqarfSvI76f6M/m1ypTFEcAIA1oJPppvYIOl/oRyX/n/2H9C//aPW2n+11n62YLe9NnWMfkKJV7bWjkjXe3H7JO9O94v531X38N75+7fW2kdaa3/RWjs43dTlf5rk/CQ3zzVD0NaCud6o39jkXslc2Fuq92pTw+nm7vea3/aO/THPT/KA1trJrbWfL2i3yT+XZZ53Yvr7tubuYZobrjc31PKDrbUfrn5VbC7BCQCYuNbaD3LNvUFPqarFnkX0azZzWN8euaY35StL7HPvzTlf8stQ9MUkD801kw/cZaDNz1prr00y1zt1903tv8q+3C93rKpFJ36oqv2S7LNg/4UW/Uz9n9FdF2k7F8TObK392nOlepvz5zLqeVfC1XOn3Yx935Cud+n3+tn+5qZ4NynEGic4AQBrxXPT3Xd043TP7rnOpnauqoflmqFcm3JRrunNuvUix7lhkqcscY7tF1ufJK21q9I9TDbpg1lVbVNV222ilo3z918jTk3yrf7rZy+xzzH9ckOSLyyxzxOrardF1j8qyU3ShYv/mrd+7llW+y72Z11V90k3vHHIqOddCXP3Yi1Wx69orZ2d5P1Jtk33rKobpOsRW4nnlzFGghMAsCa01k5N96DWluTwJF/pZ7G7/tw+VbVrVT2oqj6e7iGhO2/GcS9JN+NckpxQVQf1x9qmqu6VbpjgUj0Ff19V76qqIxbUsVdVvSrdvU8tyYf7Tbsk+VZVPaeqbl1V2y4414v6/T44fEVWRz987Ln92wdU1auravckqard+8/5v/rtz+1nq1vMdZJ8oKoO7Nteq6oek+T4fvvrW2vfm7f/Z5Jclu5+nzf1AXZu9sPHJfnPXDNpyKaMet6VMDcb4YP6qcWHzE0SMTfN+ltaa1cstTNrw6b+RwQAYFW11l5fVecl+Zck+6ebxS5VdUm6gDI/KH03ycc289BPS/LxdD1OX6mqS9P9B/IO6e6xeVyumSp6vu3STSbx4L6Oi9KFrPl1PLe1dtq89zdN9zykFya5oqouTjdb3Lb99u9k83rKVk1r7e1Vdeskz0ny50meVFUXpqt77j/aj22tvXUTh3lSkn9N8v/6tjukmxQj6YLrr3zm1toFVfWsJK9MN+zxoX27HdNd91PTDV971UD5I513hbw5yTPTDdk8t6rOSdcb+YPW2mLDOE9M8qNccw+WYXpTQI8TALCmtNbek24ChSenu+/pB+l+kd4u3VCxd6V77s0tN/eZN621z6ebjOA9SX6W5FpJzkkX0A5K8j9LNH1Fkqemm03vzHSh6dpJvp+ux+turbW/n7f/RUn+MN0sfl9INwRr53TTiH8xXTA5qL+na01prT03yb3SfdZz0812d166IWT3bq09a+AQpyS5Q5J3pBty2ZJ8I8nfJrlH3/O38JyvSvKgXNP7tF2SM5I8L8md0k1NPmTk845ba+2MdLMofiDdEMS90wXoRWdP7GdAnHvo8hcXBG/WqJrMw7kBAGDrVVVnJtk3yRNba8cP7c/kCU4AALCK+vvdPpKuJ/JGrbWLBpqwBhiqBwAAq6Sq9kjykv7tCULT9NDjBAAAK6yqXprkYenuf7pWuvvIfqu1ds5EC2Oz6XECAICVt0e650ptTPKhJPcUmqaLHicAAIABepwAAAAGCE4AAAADBCcAAIAB2026gJXyu9s81M1bAGvch69+Z026BgDYHHqcAAAABsxsjxMArKSqOivJLkk2TLgUAJa2LslFrbWbbemBBCcAWJ5ddthhh+sfcMAB1590IQAs7vTTT8/GjRvHcizBCQCWZ8MBBxxw/fXr10+6DgCWcMghh+TLX/7yhnEcyz1OAAAAAwQnAACAAYITAADAAMEJAABggOAEAAAwQHACAAAYIDgBAAAMEJwAAAAGCE4AAAADBCcAAIABghMAAMAAwQkAAGCA4AQAADBAcAIAABiw3aQLAIBpddrZF2bd0SdOtIYNxx4+0fMDbC30OAEAAAwQnAAAAAYITgAAAAMEJwAAgAGCEwAAwADBCQAAYIDgBMDMqs7jqupzVXVxVV1WVV+pqqdW1baTrg+A6SE4ATDL/i3J65PcLMnbk/xrku2TvDLJ26uqJlgbAFPEA3ABmElVdUSSI5OcleT2rbVz+/XXSvKOJA9O8pgkb5xUjQBMDz1OAMyqB/XLl82FpiRprV2R5G/6t09Z9aoAmEqCEwCzau9++Z1Fts2tO7iqdlulegCYYoITALNqrpfpZotsu/m8r/dfhVoAmHLucQJgVr0vyf9K8vSqeltr7fwkqartkjx/3n7X29RBqmr9EpsELoCtiOAEwKx6W5JHJfn9JF+vqv+b5LIk905yiyTfTLJvkqsmViEAU0NwAmAmtdaurqr7J/mLdLPrHZnkiiSnpJtN7zXpgtM5A8c5ZLH1fU/UweOsGYC1S3ACYGa11q5M8rL+9UtVtUOSg5JsTPK1CZQGwJQxOQQAW6Mjk1wnyTv66ckBYJMEJwBmVlXtssi62yU5NsklSV6w6kUBMJUM1QNgln24qjYmOS3JxUl+K8kfJLk8yYNaa4s94wkAfo3gBGyx7W6+buQ2Dz/p0yO3+eez7j5ym11+/9sjt2GmvCvJI9LNrrdDkh8meV2SY1trGyZYFwBTRnACYGa11l6S5CWTrgOA6eceJwAAgAGCEwAAwADBCQAAYIDgBAAAMEBwAgAAGGBWPQBYpgP32TXrjz180mUAsAr0OAEAAAwQnAAAAAYITgAAAAMEJwAAgAGCEwAAwACz6gG/apttR27yrX/YZeQ2j97l3JHbvGj9niO32SXfHrkNbK7Tzr4w644+cdXPu8FMfgCrTo8TAADAAMEJAABggOAEAAAwQHACAAAYIDgBAAAMEJwAAAAGCE4AzLSqOryqPlRVP6iqjVX1nap6Z1XdcdK1ATA9BCcAZlZVvTjJ+5IcnOQDSV6Z5MtJHpDkM1X1qAmWB8AU8QBcAGZSVe2d5JlJfpLkNq21c+ZtOyzJx5K8IMlbJlMhANNEjxMAs+qm6f6d+/z80JQkrbWPJ7k4yQ0mURgA00dwAmBWfTPJL5Lcvqr2mL+hqu6WZOckH5lEYQBMH0P1AJhJrbXzq+qvk7w8yder6j1JzktyiyT3T/LhJH86wRIBmCKCE/ArznzNISO3Oeuur12BSn7dXl+6alXOw+xorR1XVRuSnJDkT+Zt+laSNy4cwreYqlq/xKb9t7xCAKaFoXoAzKyq+t9J3pXkjel6mnZMckiS7yR5a1X9n8lVB8A00eMEwEyqqnskeXGSd7fWnj5v05er6oFJzkzyjKo6vrX2naWO01pbtBu274k6eIwlA7CG6XECYFb9Yb/8+MINrbXLknwh3b+Dt13NogCYToITALPq2v1yqSnH59b/YhVqAWDKCU4AzKqT++UTqmqf+Ruq6veT3DnJz5OcstqFATB93OMEwKx6V7rnNN07yelV9e4kP05yQLphfJXk6NbaeZMrEYBpITgBMJNaa1dX1R8keXKSRyR5YJLrJjk/yUlJXtVa+9AESwRgighOAMys1toVSY7rXwCwbO5xAgAAGCA4AQAADBCcAAAABghOAAAAA0wOATPsokceOnKbU/7wpcs4004jt7jZSY8fuc0tTzx15DZt5BYAAL9OcAKAZTpwn12z/tjDJ10GAKvAUD0AAIABghMAAMAAwQkAAGCA4AQAADBAcAIAABhgVj0AWKbTzr4w644+cdJlLGqD2f4AxkqPEwAAwADBCQAAYIDgBAAAMEBwAgAAGCA4AQAADDCrHkyJix556Mht3nnsS0duc8Ptdhq5zW9+4qiR2+z3hK+M3KZdfdXIbQAAxkGPEwAzqaqOqqo28JLGAdgsepwAmFWnJnn+EtvumuSeSd6/euUAMM0EJwBmUmvt1HTh6ddU1Wf7L1+7ehUBMM0M1QNgq1JVByY5NMnZSU6ccDkATAnBCYCtzZ/2y9e31tzjBMBmEZwA2GpU1Q5JHpXk6iSvm3A5AEwR9zgBsDV5WJLdkpzYWvv+5jSoqvVLbNp/bFUBsObpcQJga/KEfvkvE60CgKmjxwmArUJV3SrJnZL8IMlJm9uutXbIEsdbn+Tg8VQHwFqnxwmArYVJIQBYNsEJgJlXVddJcmS6SSFeP+FyAJhCghMAW4OHJrlekpM2d1IIAJjPPU4wCdtsO3KTRz7n/SO3ufF2O43c5h8vuMnIbW75rPNGbnPl1UZKsarmJoV47USrAGBq6XECYKZV1QFJ7pIRJ4UAgPn0OAEw01prpyepSdcBwHTT4wQAADBAcAIAABggOAEAAAwQnAAAAAYITgAAAAPMqgcAy3TgPrtm/bGHT7oMAFaBHicAAIABghMAAMAAwQkAAGCA4AQAADDA5BAwAWf+0yEjt3nK9V47cpsLr944cpv3PPHeI7fZ5rtfGbkNAMA0EZwAYJlOO/vCrDv6xImce4PZ/ABWlaF6AAAAAwQnAACAAYITAADAAMEJAABggOAEAAAwQHACAAAYIDgBMPOq6q5V9Z9V9aOqurxffqiq/mDStQEwHTzHCYCZVlXPTfJ3Sc5N8r4kP0qyR5LbJrlHkpMmVhwAU0NwAmBmVdVD04WmjyR5UGvt4gXbrzWRwgCYOobqATCTqmqbJC9OclmSRy4MTUnSWrti1QsDYCrpcQJgVt0pyc2SvCvJz6rq8CQHJvl5ki+01j47yeIAmC6CE4zBWcfecaT9T/6DlyzjLDuN3OLuL37GyG32+uQpI7eBNep2/fInSb6c5NbzN1bVp5I8pLX209UuDIDpIzgBMKv27Jd/luSsJPdO8vkkN03ysiS/l+Sd6SaIWFJVrV9i0/5jqRKAqeAeJwBm1bb9stL1LH20tXZJa+1rSR6Y5AdJ7l5Vo3UZA7BV0uMEwKz6Wb/8Tmvtf+ZvaK1trKoPJvnjJLdPsuT9Tq21QxZb3/dEHTymWgFY4/Q4ATCrvtEvL1hi+1yw2mEVagFgyglOAMyqTyW5Msm+VbX9ItsP7JcbVq0iAKaW4ATATGqtnZvk7Ul2TfK387dV1e+mmxziwiQfWP3qAJg27nECYJY9Pckdkjynqu6W5AvpZtV7YJKrkvxJa22poXwA8EuCEwAzq7V2TlXdIclz04WlQ5NcnOTEJP/QWvvcJOsDYHoITgDMtNba+el6np4+6VoAmF7ucQIAABggOAEAAAwQnAAAAAa4xwkW2HjE7Udu8/k/etlI+19v251GPse+nzhq5Da3OP5LI7dpI7cAAJh9epwAAAAG6HECgGU6cJ9ds/7YwyddBgCrQI8TAADAAMEJAABggOAEAAAwQHACAAAYIDgBAAAMMKseACzTaWdfmHVHnziRc28wmx/AqtLjBAAAMEBwAgAAGCA4AQAADBCcAAAABpgcAha43ws/OnKb62173ZH2f+2FNxr5HPv99bkjt7nyil+M3AYAgF+nxwkAAGCA4ATAzKqqDVXVlnj9eNL1ATA9DNUDYNZdmOS4RdZfstqFADC9BCcAZt0FrbVjJl0EANPNUD0AAIABepwAmHXXrqpHJfmNJJcm+WqST7XWrppsWQBME8EJgFm3d5I3L1h3VlU9trX2yUkUBMD0EZwAmGVvSHJykq8luTjJzZP8eZInJHl/Vd2xtfY/mzpAVa1fYtP+4ywUgLVNcAJgZrXWnr9g1WlJ/qyqLknyjCTHJHngatcFwPQRnADYGh2fLjjdbWjH1tohi63ve6IOHnNdAKxRZtUDYGt0Tr/ccaJVADA1BCcAtkZ37JffmWgVAEwNQ/WYad99wR2Hd1rgr67/zyO3+dlVl420/7se97sjn6O+v8n718dm2+tdb+Q2P3zUASO3ueGnfjZym/a1b47e5sorR27DbKiq30ryo9ba+QvW3zTJa/q3b1n1wgCYSoITALPqoUmOrqqPJzkr3ax6t0hyeJLrJDkpyUsnVx4A00RwAmBWfTzJLZPcNt3QvB2TXJDk0+me6/Tm1lqbXHkATBPBCYCZ1D/c1gNuARgLk0MAAAAMEJwAAAAGCE4AAAADBCcAAIABghMAAMAAs+oBwDIduM+uWX/s4ZMuA4BVoMcJAABggOAEAAAwwFA9psa2e+w+cpt3PPoVyzjTdUZucfu3PmOk/W/+2c+OfI7l2O4mNx65zY3e9bOR25x0k38auU2eNXqTg/7hSSO32evVp4x+IgCABfQ4AQAADBCcAAAABhiqBwDLdNrZF2bd0SdOuoxf2mCGP4AVo8cJAABggOAEAAAwQHACAAAYIDgBAAAMEJwAAAAGCE4AAAADBCcAtipVdWRVtf71+EnXA8B0EJwA2GpU1U2SvDrJJZOuBYDpIjgBsFWoqkryhiTnJTl+wuUAMGW2m3QBbJ222Xnnkdvs9t6rR25zm+2vM3Kbj27cduQ2+x779ZH2v2rkMyTb3uAGI7d5+Ic/N3KbR+9y7shtrmqj/9lsW6P/v83lu4/cBOZ7apJ7JrlHvwSAzabHCYCZV1UHJDk2yStba5+adD0ATB/BCYCZVlXbJXlzku8lefaEywFgShmqB8Cs+9skt01yl9baxlEbV9X6JTbtv0VVATBV9DgBMLOq6vbpeple1lr77KTrAWB66XECYCbNG6J3ZpK/We5xWmuHLHH89UkOXu5xAZguepwAmFU7JdkvyQFJfj7vobctyfP6ff61X3fcxKoEYCrocQJgVl2e5PVLbDs43X1Pn07yjSSG8QGwSYITADOpnwji8Yttq6pj0gWnf2utvW416wJgOhmqBwAAMEBwAgAAGCA4AbDVaa0d01orw/QA2FyCEwAAwACTQzARv7jdfiO3+febLTU51ng9/dV/OnKbvS84ZaT9t73BDUY+x5+d8pmR29xzh/NHbnPA8U8fuc3Od/jpyG2+cNt3jtwGAGBS9DgBAAAMEJwAAAAGCE4AAAAD3OMEAMt04D67Zv2xh0+6DABWgR4nAACAAYITAADAAMEJAABggOAEAAAwQHACAAAYYFY9AFim086+MOuOPnHSZWSDmf0AVpweJwAAgAGCEwAAwABD9ZiIi55+8aqc59k/uc3IbfZ+1edHbrPtXnuOtP+691008jnuv+NlI7e51SmPH7nNjT5z+chtPvCnbxu5zT9esG7kNjd75ekjt7lq5BYAAL9OjxMAAMAAwQkAAGCA4AQAADBAcAJgZlXVi6vqo1X1/araWFXnV9VXqup5VbX7pOsDYHoITgDMsqcl2THJh5O8Mslbk1yZ5JgkX62qm0yuNACmiVn1AJhlu7TWfr5wZVW9KMmzkzwryZNWvSoApo4eJwBm1mKhqfeOfrnvatUCwHQTnADYGt2vX351olUAMDUM1QNg5lXVM5PslGTXJL+T5C7pQtOxk6wLgOkhOAGwNXhmkr3mvf9AkqNaaz8dalhV65fYtP84CgNgOhiqB8DMa63t3VqrJHsneVCSmyf5SlUdPNnKAJgWepwA2Gq01n6S5N1V9eUkZyZ5U5IDB9ocstj6vidK8ALYSghObLFtDhx9tMonD3rTMs60/cgtPvDaO4/c5gZXf3bkNt941s1H2v+kfY4f+RzLsdMOl4/c5i+O/6+R25x5xS9GbvOff/l7I7e51s++NHIbWExr7btV9fUkB1XVHq21cyddEwBrm6F6AGytbtQvr5poFQBMBcEJgJlUVftX1d6LrN+mfwDunklOaa39bPWrA2DaGKoHwKy6b5KXVNWnknw7yXnpZta7e7rJIX6c5E8mVx4A00RwAmBWfSTJa5PcOclvJ9ktyaXpJoV4c5JXtdbOn1x5AEwTwQmAmdRaOy3JkyddBwCzwT1OAAAAAwQnAACAAYITAADAAMEJAABggOAEAAAwwKx6ALBMB+6za9Yfe/ikywBgFehxAgAAGKDHiS22zXkXjNzm0z/fceQ297nuFSO3Oe6v/nnkNk847MiR25xx538cscW2I59jOb5w23eO3ObEy64zcpvHvOjpI7fZ/UOfHbkNAMCk6HECAAAYIDgBAAAMEJwAAAAGuMcJAJbptLMvzLqjT5xoDRvM6gewKvQ4AQAADBCcAAAABghOAAAAAwQnAACAAYITAADAAMEJAABggOAEwEyqqt2r6vFV9e6q+lZVbayqC6vq01X1x1Xl30AANpvnOAENajn5AAAPb0lEQVQwqx6a5J+T/CjJx5N8L8leSR6U5HVJfr+qHtpaa5MrEYBpITixxa780Y9HbvOslzx+5Da3e+7LRm5zt+tcd+Q2Z9zlzSO3SbZdRpvRfO7nV43c5sjP/fHIbfZ96vdHbrP7uZ8duQ2sgjOT3D/Jia21q+dWVtWzk3whyYPThaj/nEx5AEwTwxQAmEmttY+11v57fmjq1/84yfH923usemEATCXBCYCt0RX98sqJVgHA1BCcANiqVNV2SR7dv/3AJGsBYHq4xwmArc2xSQ5MclJr7YNDO1fV+iU27T/WqgBY0/Q4AbDVqKqnJnlGkjOSHDnhcgCYInqcANgqVNWTk7wyydeT3Ku1dv7mtGutHbLE8dYnOXh8FQKwlulxAmDmVdVfJnlNktOSHNbPrAcAm01wAmCmVdVfJ3lFklPThaZzJlwSAFNIcAJgZlXV36SbDGJ9uuF55064JACmlHucAJhJVfWYJC9IclWSk5M8taoW7rahtfbGVS4NgCkkOAEwq27WL7dN8pdL7PPJJG9clWoAmGqG6gEwk1prx7TWauB1j0nXCcB00OPEROzxL58duc3Dvvakkdtcvvv2I7d51LHvG7nNE3b94Uj7H/L8J458jr3e/vWR29z8glNHbnPVyC0AAGafHicAAIABghMAAMAAwQkAAGCA4AQAADDA5BAAsEwH7rNr1h97+KTLAGAV6HECAAAYIDgBAAAMEJwAAAAGCE4AAAADBCcAAIABZtUDgGU67ewLs+7oEydawwaz+gGsCj1OAAAAA/Q4MTW2+fSpI7fZYRnnOWGn+4/c5uEvfsVI++918nkjn+OqCy4cuQ0AAOOhxwkAAGCA4AQAADBAcAIAABggOAEAAAwQnACYWVX1kKp6dVWdXFUXVVWrqrdMui4Apo9Z9QCYZc9N8ttJLknygyT7T7YcAKaVHicAZtnTkuyXZJckT5xwLQBMMT1OAMys1trH576uqkmWAsCU0+MEAAAwQHACAAAYYKgeAGxCVa1fYpOJJgC2InqcAAAABuhxggV2fevnRm7zsLfeccQWZ458DmAyWmuHLLa+74k6eJXLAWBC9DgBAAAMEJwAAAAGCE4AAAAD3OMEwMyqqiOSHNG/3btf3rGq3th/fW5r7ZmrXhgAU0dwAmCWHZTkMQvW3bx/Jcl3kwhOAAwyVA+AmdVaO6a1Vpt4rZt0jQBMB8EJAABggOAEAAAwQHACAAAYIDgBAAAMEJwAAAAGmI4cAJbpwH12zfpjD590GQCsAj1OAAAAAwQnAACAAYITAADAAMEJAABggOAEAAAwwKx6ALBMp519YdYdfeKky8gGM/sBrDg9TgAAAAMEJwAAgAGCEwAAwADBCQAAYIDgBAAAMEBwAgAAGCA4ATDTqurGVXVCVf2wqi6vqg1VdVxVXW/StQEwPTzHCYCZVVW3SHJKkj2TvDfJGUlun+Qvkty3qu7cWjtvgiUCMCX0OAEwy/4pXWh6amvtiNba0a21eyZ5RZJbJnnRRKsDYGoITgDMpKq6eZL7JNmQ5B8XbH5ekkuTHFlVO65yaQBMIcEJgFl1z375odba1fM3tNYuTvKZJNdNcuhqFwbA9BGcAJhVt+yXZy6x/Zv9cr9VqAWAKWdyCABm1a798sIlts+t321TB6mq9Uts2n85RQEwnfQ4AbC1qn7ZJloFAFNBjxMAs2quR2nXJbbvsmC/RbXWDllsfd8TdfDySgNg2uhxAmBWfaNfLnUP0779cql7oADglwQnAGbVx/vlfarqV/69q6qdk9w5ycYkn1vtwgCYPoITADOptfbtJB9Ksi7Jkxdsfn6SHZO8qbV26SqXBsAUco8TALPsSUlOSfKqqrpXktOT3CHJYemG6D1ngrUBMEX0OAEws/pep99J8sZ0gekZSW6R5FVJ7thaO29y1QEwTfQ4ATDTWmvfT/LYSdcBwHTT4wQAADBAcAIAABggOAEAAAwQnAAAAAYITgAAAAPMqgcAy3TgPrtm/bGHT7oMAFaBHicAAIABghMAAMAAwQkAAGCA4AQAADBAcAIAABggOAEAAAwQnAAAAAYITgAAAAMEJwAAgAGCEwAAwADBCQAAYIDgBAAAMEBwAgAAGCA4AQAADNhu0gUAwJRad/rpp+eQQw6ZdB0ALOH0009PknXjOJbgBADLs9PGjRuv+vKXv/w/ky5kwvbvl2dMtIrJcx06rkPHdeisheuwLslF4ziQ4AQAy3NakrTWtuoup6pan7gOrkPHdei4Dp1Zuw7ucQIAABggOAEAAAyY2aF6H776nTXpGgAAgNmgxwkAAGCA4AQAADCgWmuTrgEAAGBN0+MEAAAwQHACAAAYIDgBAAAMEJwAAAAGCE4AAAADBCcAAIABghMAAMAAwQkAelV146o6oap+WFWXV9WGqjquqq434nGu37fb0B/nh/1xb7xStY/Tll6Hqtqxqv6oqv69qs6oqkur6uKq+lJVPaOqtl/pzzAO4/p+WHDMu1XVVVXVquqF46x3pYzzOlTVravqTVX1/f5Y51TVJ6vq0StR+ziN8efDXarqvX37n1fV96rqpKq670rVPi5V9ZCqenVVnVxVF/Xfx29Z5rHG/vdrpXkALgAkqapbJDklyZ5J3pvkjCS3T3JYkm8kuXNr7bzNOM7u/XH2S/KxJF9Msn+SByQ5J8kdW2vfWYnPMA7juA79L4DvT3J+ko8n+VaS6ye5X5K9++Pfq7X28xX6GFtsXN8PC465c5KvJtkjyU5JXtRae+446x63cV6HqjoqyeuSXJbkfUk2JNktyYFJfthae8SYyx+bMf58eGKSf0pyaZJ3J/lBkhsneVCS6yZ5bmvtRSvxGcahqk5N8ttJLklX+/5J3tpae9SIxxn7369V0Vrz8vLy8vLa6l9JPpikJXnKgvUv79cfv5nH+Zd+/5cvWP/Ufv0HJv1ZV/o6JDkoyR8l2X7B+p2TrO+P84xJf9bV+H5Y0PaEdGHy2f0xXjjpz7la1yHJoUmuTHJqkr0X2X6tSX/Wlb4OSa6V5IIkG5PccsG2A5L8PF2ovPakP+8mPsNhSfZNUknu0X/2t0zq+2q1X3qcANjqVdXNk3w73f+A36K1dvW8bTsn+VG6XxT2bK1duonj7Jjkp0muTnLD1trF87Zt059jXX+ONdfrNK7rMHCORyZ5a5L3tdbut8VFr4CVuA5V9YAk70lyZJLtkrwha7zHaZzXoao+leSuSW7dWjttxYpeAWP8+bBXkh8n+Wpr7bcX2f7VJLdOskdbi70tC1TVPdL1KI/U47QaP2dWinucACC5Z7/80Px/xJOkDz+fSTeM5tCB49wxyQ5JPjM/NPXHuTrJh/q3h21xxStjXNdhU67ol1duwTFW2livQ1XtmeRfk7yntbas+0EmZCzXob+3765JvpTka1V1WFU9s7/f7V79fyqsZeP6fjgn3X+s7FdV+87fUFX7pevJOXUaQtMWWo2fMytirX+jAsBquGW/PHOJ7d/sl/ut0nEmZTXqf1y//MAWHGOljfs6vDbd71x/tiVFTcC4rsPt5u3/sf71kiQvTfKRJKdW1W9uQZ0rbSzXoXXDvJ6c7nthfVX9W1X9Q1W9Kd0Q1q8leegY6l3rpvbn5HaTLgAA1oBd++WFS2yfW7/bKh1nUla0/qr68yT3TXefywnLOcYqGdt1qKrHpZsY5OGttZ+MobbVNK7rsGe/fFiSc9NNhPDRJDdI8rx0wxdPrKpbt9Z+sfxyV8zYvh9aa++sqh8m+Y8k82cS/Em64ZtrbgjvCpjan5N6nABgWPXLLb0xeFzHmZRl119VD0pyXLp7PB7cWrtioMlatlnXoarWpfvM72ytvWOFa5qEzf1+2Hbe8vGttXe31i5qrX07yWPSDeHbL8mDV6bMFbfZfy+q6lHpetlOTjchxHX75UeTvCbJ21aoxmmyZn9OCk4AcM3/cO66xPZdFuy30seZlBWpv6qOSPcL4TlJ7rEWJ8ZYYFzX4YR0M6g9aRxFTcC4rsPP+uXlSU6av6Efvvbe/u3tRy1wlYzlOvT3MZ2Qbkjeka21M1prG1trZ6TrdVuf5KH9pAuzbGp/TgpOANA9NyRZekz93I3cS43JH/dxJmXs9VfVQ5O8M91QpLu31r4x0GQtGNd1ODjdMLWf9g8KbVXV0g3JSpLn9Oves2Xlrphx/724eOFkAL25YLXDCLWtpnFdh/ukm5L8k4tMinB1kk/1bw9ZTpFTZGp/TrrHCQC6KXWT5D5Vtc0i0+PeOV3PwecGjvO5fr87V9XOi0xHfp8F51trxnUd5to8Msmbkpyd5LAp6GmaM67r8KZ0Q7EW2jfJ3dLd67U+yVe2uOKVMa7r8NV09zbtUVV7LXKv14H9csOWl7wixnUdrt0vb7DE9rn1a/E+r3Ea68+Z1aTHCYCtXn+vxYfSPWPpyQs2Pz/JjkneNP+ZIlW1f1Xtv+A4lyR5c7//MQuO8+f98T+4VgPEuK5Dv/4x6a7F95Lcba1+5sWM8fvhqa21xy985ZoepxP7df+4Yh9mC4zxOlyZ7sHQSfJ/5k8/XlW3TnJUuunp3zXmjzAWY/x7cXK/fEhV3Wb+hqo6KMlD0t3X87HxVT85VXWt/jrcYv765VzPtcIDcAEgSf+P+ynphla9N8npSe6Q7plLZya50/znq/RDrtJaqwXH2b0/zn7pfgH6Qrqbvx+Q7h6fO/W/OKxJ47gOVXVYuhvgt0l3T8f3FznVBa2141boY2yxcX0/LHHsozIFD8BNxvr34rrpJkA4NF0P2yfS9bA8ON0QvWe01l6+wh9n2cZ4HU5I8th0vUrvTvLddAHiiCTbJzmutfa0Ff44y9bfr3hE/3bvJL+XbibAuVB4bmvtmf2+65KcleS7rbV1C44z0vVcKwQnAOhV1U2SvCDdlNm7p3uC/XuSPL+1dv6CfZf8Rbmqrp9umuUjktwwyXlJ3p/kb1trP1jJzzAOW3od5gWDTfm1X6bWmnF9Pyxy3KMyJcEpGevfi+sm+d9JHpHkZkl+nuSLSV7WWnv/Sn6GcRjHdaiqSjeT4FFJfjvJzkkuShcm/7W1tqZn1auqY9L9bFvKL/9ebyo49ds3+3quFYITAADAAPc4AQAADBCcAAAABghOAAAAAwQnAACAAYITAADAAMEJAABggOAEAAAwQHACAAAYIDgBAAAMEJwAAAAGCE4AAAADBCcAAIABghMAAMAAwQkAAGCA4AQAADBAcAIAABggOAEAAAwQnAAAAAYITgAAAAMEJwAAgAGCEwAAwID/D/3X9J/3ZTORAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 226,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.7996e-03, -1.5700e-02,  9.6686e-03,  ...,  1.3861e-02,\n",
       "          1.9976e-03, -2.9913e-02],\n",
       "        [-1.4090e-02, -2.6841e-02,  6.2824e-03,  ..., -6.6598e-03,\n",
       "          9.6329e-03,  7.3714e-03],\n",
       "        [-1.0180e-02,  1.4984e-02,  1.8337e-02,  ..., -2.1607e-02,\n",
       "          3.0454e-03, -2.7341e-02],\n",
       "        ...,\n",
       "        [ 1.1482e-02, -2.5240e-02,  1.1014e-03,  ..., -1.3786e-03,\n",
       "          2.5028e-02, -9.9910e-04],\n",
       "        [ 1.8222e-02,  1.5144e-02,  3.5190e-02,  ...,  6.7634e-03,\n",
       "         -8.0357e-03, -7.7583e-03],\n",
       "        [-9.3491e-03, -3.3898e-02, -1.6165e-03,  ..., -1.1142e-02,\n",
       "         -2.4027e-02, -2.2081e-02]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Note:** I'm glossing over a few details here that require some knowledge of vector calculus, but they aren't necessary to understand what's going on.\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktionen in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `nn` module, PyTorch provides losses such as the cross-entropy loss (`nn.CrossEntropyLoss`). You'll usually see the loss assigned to `criterion`. As noted in the last part, with a classification problem such as MNIST, we're using the softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss. To actually calculate the loss, you first define the criterion then pass in the output of your network and the correct labels.\n",
    "\n",
    "Something really important to note here. Looking at [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    ">\n",
    "> The input is expected to contain scores for each class.\n",
    "\n",
    "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the *logits* or *scores*. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3231)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3160)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Autograd: Berechnung der lokalen Ableitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0887, -0.2961],\n",
      "        [-1.3091, -1.2157]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0079,  0.0877],\n",
      "        [ 1.7137,  1.4780]])\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CNN in pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation\n",
    "\n",
    "Installation can be easily done through the official website:  https://pytorch.org/. Simply select OS, python version and if you want the GPU (with CUDA) or the CPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 0.4.1\n",
      "CUDA is active: True\n",
      "CUDA version: 9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA is active:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Fundamentals - Matrices and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy vs. PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "Torch array:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Numpy array of ones:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "Torch array of ones:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "Numpy array of random int:\n",
      " [[0.22712468 0.1864801 ]\n",
      " [0.98675439 0.86452921]]\n",
      "Torch array of random int:\n",
      " tensor([[0.6608, 0.7621],\n",
      "        [0.0416, 0.7732]])\n"
     ]
    }
   ],
   "source": [
    "#### With values ####\n",
    "#Python array\n",
    "arr = [[1,2], [3,4]]\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Bidimensional array\n",
    "numpy_arr = np.array([[1,2], [3,4]])\n",
    "print(\"Numpy array:\\n\", numpy_arr)\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "arr_t = torch.Tensor(arr)#Conversion to torch array\n",
    "print(\"Torch array:\\n\", arr_t)\n",
    "##### With default values #####\n",
    "print(\"Numpy array of ones:\\n\", np.ones((2,2)))\n",
    "print(\"Torch array of ones:\\n\", torch.ones((2,2)))\n",
    "\n",
    "#### With random values ####\n",
    "print(\"Numpy array of random int:\\n\", np.random.rand(2,2))\n",
    "print(\"Torch array of random int:\\n\", torch.rand(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility with Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds in numpy:\n",
      "\n",
      "[[0.5488135  0.71518937]\n",
      " [0.60276338 0.54488318]]\n",
      "tensor([[0.4963, 0.7682],\n",
      "        [0.0885, 0.1320]])\n",
      "tensor([[0.3074, 0.6341],\n",
      "        [0.4901, 0.8964]])\n"
     ]
    }
   ],
   "source": [
    "######## Seeds in numpy \n",
    "print(\"Seeds in numpy:\\n\")\n",
    "np.random.seed(0)\n",
    "print(np.random.rand(2,2))\n",
    "\n",
    "####### Seeds in torch (CPU)\n",
    "torch.manual_seed(0)\n",
    "print(torch.rand(2,2))\n",
    "\n",
    "###### Seeds in torch (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    print(torch.rand(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy-PyTorch Bridge\n",
    "\n",
    "Torch supports following data types:\n",
    "double, float, float16, int64, int32, and uint8.\n",
    "//TODO: Inserisci tabella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From NumPy to Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Numpy array\n",
    "np_array = np.ones((2,2))\n",
    "\n",
    "#Torch tensor\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types matter in pyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: can't convert np.ndarray of type numpy.int8. The only supported types are: double, float, float16, int64, int32, and uint8.\n"
     ]
    }
   ],
   "source": [
    "#Data types matter!\n",
    "np_array2 = np.ones((2,2), dtype=np.int8)\n",
    "try:\n",
    "    torch.from_numpy(np_array2)\n",
    "except TypeError as e:\n",
    "    print(\"TypeError:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array3 = np.ones((2,2), dtype=np.int64)\n",
    "torch.from_numpy(np_array3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Torch tensor\n",
    "torch_tensor2 = torch.ones(2,2)\n",
    "print(type(torch_tensor2))\n",
    "torch_to_numpy = torch_tensor2.numpy()\n",
    "print(type(torch_to_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tesors on CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU\n",
    "tensor_cpu = torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#CPU to GPU\n",
    "if torch.cuda.is_available():\n",
    "    tensor_cpu.cuda()\n",
    "    print(tensor_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPU to CPU\n",
    "tensor_cpu.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dimension:\n",
      "shape variable: torch.Size([2, 2])\n",
      "size()-method torch.Size([2, 2])\n",
      "Resized tensor: tensor([1., 1., 1., 1.])\n",
      "Dimension: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "#### Resizing a tensor\n",
    "a = torch.ones(2,2)\n",
    "print(\"Tensor dimension:\")\n",
    "print(\"shape variable:\", a.shape)\n",
    "print(\"size()-method\", a.size())\n",
    "\n",
    "### view()\n",
    "print(\"Resized tensor:\", a.view(4))\n",
    "print(\"Dimension:\", a.view(4).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "A:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "A:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.ones(2,2)\n",
    "\n",
    "### Addition (element-wise)\n",
    "add = a+b \n",
    "add2 = torch.add(a,b)\n",
    "print(add)\n",
    "print(add2)\n",
    "\n",
    "#In-place addition\n",
    "print()\n",
    "add.add_(a)\n",
    "print(add)\n",
    "\n",
    "#### Subtraction\n",
    "print(a-b)\n",
    "print(a.sub(b))\n",
    "print(\"A:\\n\",a)\n",
    "print()\n",
    "#In place\n",
    "print(a.sub_(b))\n",
    "print(\"A:\\n\",a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients - Parameter \"requires_grad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating a tensor with gradient which allow the accumulation of gradients\n",
    "a_grad = torch.ones((2,2), requires_grad=True)\n",
    "a_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<ThAddBackward>)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Behaves similarly to tensors\n",
    "b_grad = torch.ones((2, 2), requires_grad=True)\n",
    "print(a_grad + b_grad)\n",
    "print(torch.add(a_grad, b_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is exactly `requires_grad`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function:\n",
    "$$y_i = 5(x_i+1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 1.], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# x value\n",
    "x = torch.ones(2, requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "y: tensor([20., 20.], grad_fn=<MulBackward>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "y_test = torch.ones(2)*5\n",
    "print(y_test.requires_grad)\n",
    "y = 5* (x + 1) ** 2\n",
    "print(\"y:\", y)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable**\n",
    "- Let's reduce y to a scalar then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$o = \\frac{1}{2}\\sum_i y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., grad_fn=<MulBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = (1/2) * torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> **Recap `y` equation**: $y_i = 5(x_i+1)^2$ </center>\n",
    "<center> **Recap `o` equation**: $o = \\frac{1}{2}\\sum_i y_i$ </center>\n",
    "<center> **Substitute `y` into `o` equation**: $o = \\frac{1}{2} \\sum_i 5(x_i+1)^2$ </center>\n",
    "$$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]$$\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.backward of tensor(20., grad_fn=<MulBackward>)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feedforward Neural Network with PyTorch (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "- **Input** dimension: **784** \n",
    "    - Size of image\n",
    "    - $28 \\times 28 = 784$\n",
    "- **Output** dimension: **10**\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "- **Hidden** dimension: **100**\n",
    "    - Can be any number\n",
    "    - Similar term\n",
    "        - Number of neurons\n",
    "        - Number of non-linear activation functions\n",
    "        \n",
    "Our model will have 1 hidden layer and **sigmoid** activation.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "- Feedforward Neural Network: **Cross Entropy Loss**\n",
    "    - _Logistic Regression_: **Cross Entropy Loss**\n",
    "    - _Linear Regression_: **MSE**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our tensors with gradient accumulation capabilities)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000021F25700C78>\n",
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())\n",
    "\n",
    "###### IMMAGINE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "- Process \n",
    "    1. Convert inputs to tensors with gradient accumulation capabilities\n",
    "    2. Clear gradient buffers\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.595485508441925. Accuracy: 86\n",
      "Iteration: 1000. Loss: 0.5494940876960754. Accuracy: 89\n",
      "Iteration: 1500. Loss: 0.45526909828186035. Accuracy: 90\n",
      "Iteration: 2000. Loss: 0.4022116959095001. Accuracy: 91\n",
      "Iteration: 2500. Loss: 0.22733356058597565. Accuracy: 91\n",
      "Iteration: 3000. Loss: 0.27196046710014343. Accuracy: 91\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen\n",
    "\n",
    "1. Deep Learning Wizard: https://www.deeplearningwizard.com/deep_learning/intro/ (images, explanations, examples..)\n",
    "2. Siraj Raval, \"Pytorch in 5 Minutes\": https://www.youtube.com/watch?v=nbJ-2G2GXL0 (very quick start)\n",
    "3. Udacity, Intro to Deep Learning with PyTorch: https://www.udacity.com/course/deep-learning-pytorch--ud188\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
