{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - Vortrag\n",
    "### HTW Berlin - Angewandte Informatik (B. Sc.)\n",
    "#### Modul \"Ausgewählte Kapitel sozialer Webtechnologien\" (aka Neuronale Netze)\n",
    "\n",
    "##### Diletta Calussi - s0559842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhalte\n",
    "\n",
    "1. Das Framework PyTorch\n",
    "2. PyTorch Fundamentals (Warm-up)\n",
    "3. Neuronale Netze in PyTorch\n",
    "4. Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Packages für die Präsentation\n",
    "- torch (CUDA oder GPU)\n",
    "- torchvision\n",
    "- numpy\n",
    "\n",
    "\n",
    "\n",
    "Eine Anleitung für die Installation ist auf der Webseite von [PyTorch](https://pytorch.org/) verfügbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Das Framework [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eine ML Open-Source-Bibliothek für python\n",
    "- Inspiriert nach der in [**Lua**](https://www.lua.org/) geschriebenen Bibliothek [**Torch**](http://torch.ch/)\n",
    "- Vom Facebook-Forschungsteam für K.I. entwickelt \n",
    "- Erscheinungsjahr: 2016\n",
    "- Unterstützt GPU sowie CPU \n",
    "\n",
    "### **Key Features**: \n",
    "\n",
    "1. Imperative Programmierung\n",
    "2. Dynamische Graphen (Dynamic Computation Graph)\n",
    "    - Autograd-System zur automatischen Berechnung von Ableitungen in einem Graphen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Feature 1: Imperative Programming\n",
    "\n",
    "* \"Tell what and tell how\"\n",
    "* vs. *symbolische Programmierung*:\n",
    "     - Zunächst die Graphstruktur wird definiert\n",
    "     - Dann wird der Graph einer Funktion übergeben\n",
    "     - Computation erfolgt am Ende (!)\n",
    "* Flexibilität:\n",
    "    - --> Key Feature 2: Dynamic Computation Graph (s.u.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 0.4.1\n",
      "CUDA is active: True\n",
      "CUDA version: 8.0\n"
     ]
    }
   ],
   "source": [
    "#Installation check\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA is active:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Fundamentals (Warm-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 [Tensoren](https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html)\n",
    "\n",
    "Ein `torch.Tensor` ist eine mehrdimensionale Matrix, die Elemente von einem bestimmten Datentyp enthält. Ein detaillierter Überblick der unterstützten Datentype ist auf der [Webseite von PyTorch](https://pytorch.org/docs/stable/tensors.html) verfügbar. \n",
    "\n",
    "PyTorch-Tensoren sind sowie numpy-Matrizen, nur dass sie GPU-Berechnungen ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiele aus der [PyTorch-Webseite](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.6567, 0.0209, 0.1491],\n",
      "        [0.0021, 0.0705, 0.2531],\n",
      "        [0.1855, 0.5600, 0.0507],\n",
      "        [0.7544, 0.1731, 0.4208],\n",
      "        [0.7196, 0.0541, 0.3483]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.5523,  0.7173,  0.2235],\n",
      "        [-1.0418, -1.0518, -0.5022],\n",
      "        [-1.9044,  1.0648,  1.8611],\n",
      "        [ 0.3088, -1.2730,  0.9823],\n",
      "        [ 0.1267, -1.2832,  0.5282]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "#5x3 matrix, nicht initialisiert\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "\n",
    "#Random-Initialisierung\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "#Matrix filled with zeros mit Typ Long\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operationen auf Tensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 1., 4.],\n",
      "        [4., 6., 5.]])\n",
      "\n",
      "tensor([[6., 2., 5.],\n",
      "        [3., 3., 3.]])\n",
      "Elementwise-Operationen:\n",
      "Addition: \n",
      " tensor([[9., 3., 9.],\n",
      "        [7., 9., 8.]]) \n",
      "oder: tensor([[9., 3., 9.],\n",
      "        [7., 9., 8.]])\n",
      "************************************************************\n",
      "Subtraktion:\n",
      " tensor([[-3., -1., -1.],\n",
      "        [ 1.,  3.,  2.]]) \n",
      "oder: tensor([[-3., -1., -1.],\n",
      "        [ 1.,  3.,  2.]])\n",
      "************************************************************\n",
      "Multiplikation:\n",
      " tensor([[18.,  2., 20.],\n",
      "        [12., 18., 15.]]) \n",
      "oder: tensor([[18.,  2., 20.],\n",
      "        [12., 18., 15.]])\n",
      "************************************************************\n",
      "Division:\n",
      " tensor([[0.5000, 0.5000, 0.8000],\n",
      "        [1.3333, 2.0000, 1.6667]]) \n",
      "oder: tensor([[0.5000, 0.5000, 0.8000],\n",
      "        [1.3333, 2.0000, 1.6667]])\n",
      "************************************************************\n",
      "IN-PLACE: \n",
      "tensor([[9., 3., 9.],\n",
      "        [7., 9., 8.]])\n",
      "tensor([[9., 3., 9.],\n",
      "        [7., 9., 8.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(1,10,(2,3))\n",
    "b = torch.randint(1,10,(2,3))\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "\n",
    "#Elementwise Operationen (mit den Funktionen .add(), .sub(), .mul(), .div())\n",
    "print(\"Elementwise-Operationen:\")\n",
    "print(\"Addition: \\n\", torch.add(a,b), \"\\noder:\", a+b)\n",
    "print(\"*\"*60)\n",
    "print(\"Subtraktion:\\n\", torch.sub(a,b), \"\\noder:\", a-b)\n",
    "print(\"*\"*60)\n",
    "print(\"Multiplikation:\\n\", torch.mul(a,b), \"\\noder:\", a*b)\n",
    "print(\"*\"*60)\n",
    "print(\"Division:\\n\", torch.div(a,b), \"\\noder:\", a/b)\n",
    "print(\"*\"*60)\n",
    "print(\"IN-PLACE: \")\n",
    "print(a.add_(b)) #adds b to a\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing geht am besten mit [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "size mismatch, m1: [2 x 3], m2: [2 x 3] at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533090265711\\work\\aten\\src\\th\\generic/THTensorMath.cpp:2070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 96.,  54.],\n",
       "        [111.,  65.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping mit view\n",
    "print(a.size(), b.size())\n",
    "try:\n",
    "    torch.mm(a,b)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "#Resizing    \n",
    "b_view = b.view(3,2)\n",
    "torch.mm(a,b_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 NumPy Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([1., 1., 1., 1., 1.]) <class 'torch.Tensor'>\n",
      "b: [1. 1. 1. 1. 1.] <class 'numpy.ndarray'>\n",
      "w: [1. 1. 1. 1. 1.] <class 'numpy.ndarray'>\n",
      "v: tensor([1., 1., 1., 1., 1.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "a: tensor([2., 2., 2., 2., 2.])\n",
      "b: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(\"a:\",a, type(a))\n",
    "\n",
    "#Conversion PyTorch-NumPy\n",
    "b = a.numpy()\n",
    "print(\"b:\", b, type(b))\n",
    "\n",
    "#Conversion NumPy - PyTorch\n",
    "w = np.ones(5)\n",
    "v = torch.from_numpy(w)\n",
    "print(\"w:\", w, type(w))\n",
    "print(\"v:\", v, type(v))\n",
    "\n",
    "#Sharing same memory locations --> Changes apply to each vector\n",
    "a.add_(1)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Key Feature 2: Dynamic Computation Graph und [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) zur automatischen Differentierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Dynamic Computation Graph**\n",
    "\n",
    "In Frameworks, die mit statischen Computation-Graphen arbeiten, wird die Graphstruktur zunächst aufgebaut, dann verwendet man eine Session, um Operation innerhalb des Graphen auszuführen.\n",
    "\n",
    "Im Gegensatz dazu, ist ein PyTorch Computation Graph zur Laufzeit erzeugt (**define-by-run**). \n",
    "- Nützlich bei sich verändernden Graphstrukturen (z.B. RNNs usw.)\n",
    "- Code ist genau so wie beim normalen Programmieren\n",
    "- Standard Loops oder If-Statements sind möglich\n",
    "\n",
    "\n",
    "**II. Autograd**\n",
    "\n",
    "Das Package `torch.autograd` bietet eine automatische Differenzierung für alle Operationen an Tensoren. \n",
    "\n",
    "Notwendig sind folgende Objekte:\n",
    "- `torch.Tensor`-Objekte mit dem Attribut `requires_grad` auf `True` gesetzt. Am Ende einer Computation reicht es aus, die Methode `backward()` aufzurufen, damit alle Gradienten automatisch berechnet werden. Der Gradient von einem Tensor kann mit dem Attribut `grad` angesehen werden\n",
    "- `Funktion`-Objekte die mit Tensoren in einem Graphen verbunden sind. Jeder Tensor hat das Attribut `.grad_fn`, das eine Referenz auf die Funktion enthält, die den Tensor generiert hat).\n",
    "\n",
    "Beispiel aus der Übung:\n",
    "\n",
    "<img src=\"graph.png\" >\n",
    "\n",
    "* a = 2\n",
    "* b = e\n",
    "* c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Modules\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(2.7183, requires_grad=True) tensor(3., requires_grad=True)\n",
      "Variable a:  2.0\n"
     ]
    }
   ],
   "source": [
    "#Tensor deklariation\n",
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(np.e, requires_grad=True)\n",
    "c = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "print(a, b, c)\n",
    "#Wenn ein Tensor nur ein Element enthält, kann das Element mit .item() ausgegeben werden\n",
    "print(\"Variable a: \", a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n",
      "True True True\n"
     ]
    }
   ],
   "source": [
    "print(a.grad, b.grad, c.grad)\n",
    "print(a.requires_grad, b.requires_grad, c.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyExerciseFunction(a,b,c):\n",
    "    ln = torch.log(b)\n",
    "    print(ln.grad_fn)\n",
    "    x = a + ln \n",
    "    print(x.grad_fn)\n",
    "    x = c * x\n",
    "    print(x.grad_fn)\n",
    "    x = (1./3.)*x\n",
    "    print(x.grad_fn)\n",
    "    out = 1./x\n",
    "    print(out.grad_fn)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LogBackward object at 0x000001F409933208>\n",
      "<ThAddBackward object at 0x000001F409933208>\n",
      "<ThMulBackward object at 0x000001F409933208>\n",
      "<MulBackward object at 0x000001F409933208>\n",
      "<MulBackward object at 0x000001F409933208>\n",
      "Ergebnis aus Fowardpass:  0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "out =  applyExerciseFunction(a,b,c) #1/3\n",
    "print(\"Ergebnis aus Fowardpass: \", out.item()) #1./3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dout/da:  tensor(-0.1111)\n",
      "dout/db:  tensor(-0.0409)\n",
      "dout/dc:  tensor(-0.1111)\n"
     ]
    }
   ],
   "source": [
    "print(\"dout/da: \", a.grad)\n",
    "print(\"dout/db: \", b.grad)\n",
    "print(\"dout/dc: \", c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Ausschalten der Gradienten:\n",
    "print(a.requires_grad)\n",
    "print((a ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((a ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neuronale Netze in PyTorch: `torch.nn` und `torchvision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Package [`torch.nn`](https://pytorch.org/docs/stable/nn.html#)\n",
    "\n",
    "Neuronale Netze können in PyTorch einfach mit dem Objekten und Funktionen aus dem Modul `torch.nn` erzeugt werden.\n",
    "\n",
    "Das Package bietet Klassen für \n",
    "* die allgemeine Definition eines Modells (sog. [Container](https://pytorch.org/docs/stable/nn.html#containers)), wie `nn.Module`, sowie dessen \n",
    "* Layers, \n",
    "* Aktivierungsfunktionen, \n",
    "* Kostenfuntkionen ([Loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions))\n",
    "\n",
    "Optimizer sind im Package [`torch.optim`](https://pytorch.org/docs/stable/optim.html#module-torch.optim) zu finden.\n",
    "Funktionen sind auch im Package `torch.nn.functional` verfügbar.\n",
    "\n",
    "Ein `nn.Module` enthält die Methode `forward(input)`, die das Ergebnis berechnet.\n",
    "\n",
    "\n",
    "### Das Package `torchvision`\n",
    "\n",
    "Viele Datensätze sind mit dem Package [`torchvision.datasets`](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision-datasets) verfügbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 FeedForward-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 MNIST-Datensatz vorbereiten\n",
    "\n",
    "Folgendes Beispiel implementiert ein FeedForward-Network für die Klassifizierung der Bilder aus dem MNIST-Datensatz.\n",
    "\n",
    "Folgendes Beispiel ist aus der Webseite \"Deep Learning Wizard\" übernommen: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dataset.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datensatz iterierbar machen\n",
    "\n",
    "- 60.000 Bilder (Trainingsproben)\n",
    "- Batch-Größe: 100\n",
    "- 5 Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "#n_iters = 3000\n",
    "#num_epochs = n_iters / (len(train_dataset) / batch_size) # 5 Epochen insgesamt\n",
    "#num_epochs = int(num_epochs)\n",
    "num_epochs = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Netzklasse mit `nn.Module` definieren\n",
    "\n",
    "PyTorch bietet viele Möglichkeiten, um ein Netz zu definieren. Hier leitet die Netzklasse aus dem Modul `nn.Module` ab. Sie definiert im Konsktruktor die notwendigen Layer sowie die Aktivierungsfunktion (== die Struktur) und in der Methode `forward()` die Schritte zum Ergebnis.\n",
    "\n",
    "Eine Liste von weiteren Aktivierungsfunktionen ist [hier](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) verfügbar.\n",
    "\n",
    "Weitere Beispiele zur Definiton von einer Netzklasse:\n",
    "- PyTorch-Doku: https://pytorch.org/docs/stable/nn.html#\n",
    "- Udacity \"Neural Networks with PyTorch\" (Jupyter Notebook):  \n",
    "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%202%20-%20Neural%20Networks%20in%20PyTorch%20(Solution).ipynb\n",
    "\n",
    "Folgende Implementierungen stammen aus dieser Webseite:\n",
    "- **Deep Learning Wizard**: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU oder CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: 1 Hidden Layer Feedforward Neural Network\n",
    "- Aktiviverungsfunktion: Sigmoid\n",
    "- Loss: Cross-Entropy mit Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        #Struktur\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht\n",
    "\n",
    "<img src=\"nn1_params3.png\" >\n",
    "\n",
    "Quelle: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellklasse instanzieren\n",
    "\n",
    "- Input dimension: 784\n",
    "    * Size of image\n",
    "        * 28×28=784\n",
    "- Output dimension: 10 (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)    \n",
    "\n",
    "- Hidden dimension: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetModel(\n",
       "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 28*28 #784\n",
    "hidden_dim = 100 #Anzahl von Neurone im hidden Layer\n",
    "output_dim = 10 #Targets\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "- Feedforward Neural Network: **Cross Entropy Loss**\n",
    "    - Berechnung des Fehlers zwishen den softmax output und den Labels\n",
    "    \n",
    "\n",
    "Achtung: \n",
    "- Die [`CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#crossentropyloss)-Funktion in PyTorch berechnet automatisch die Softmax-Werte. Deswegen muss man beim letzten Schritt im Forward-Pass kein Softmax berechnen    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- Stochastich Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter von einem Netz untersuchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell trainieren\n",
    "\n",
    "1. Input redimensionieren und mit requires_grad versehen\n",
    "2. Gradient-Buffer zurücksetzen: `zero_grad()`\n",
    "3. Berechnung des Outputs \n",
    "4. Berechnung des Fehlers durch Anwendung des `criterion` auf die berechneten Ergebnisse und bestehenden Labels\n",
    "5. Berechnung der Gradienten bezüglich der Parameter: `backward()`\n",
    "6. Parameter über den Optimizer aktualisieren: `step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.5622904300689697. Accuracy: 86.26%\n",
      "Iteration: 1000. Loss: 0.46150150895118713. Accuracy: 89.50%\n",
      "Iteration: 1500. Loss: 0.3779106140136719. Accuracy: 90.36%\n",
      "Iteration: 2000. Loss: 0.47078558802604675. Accuracy: 91.12%\n",
      "Iteration: 2500. Loss: 0.3569553792476654. Accuracy: 91.72%\n",
      "Iteration: 3000. Loss: 0.1626761555671692. Accuracy: 92.23%\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images) #784\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step() #parameters = parameters - learning_rate * parameters_gradients\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "    \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0) #100\n",
    "\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {:.2f}%'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell B: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3413926661014557. Accuracy: 91.35%\n",
      "Iteration: 1000. Loss: 0.24208097159862518. Accuracy: 93.18%\n",
      "Iteration: 1500. Loss: 0.23489351570606232. Accuracy: 94.13%\n",
      "Iteration: 2000. Loss: 0.1471656858921051. Accuracy: 94.64%\n",
      "Iteration: 2500. Loss: 0.1271606981754303. Accuracy: 95.47%\n",
      "Iteration: 3000. Loss: 0.1930629312992096. Accuracy: 95.78%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0) #jedes Mal um 100 erhöht\n",
    "                \n",
    "                ######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {:.2f}%'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "Quelle: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.4178018569946289. Accuracy: 90.73%\n",
      "Iteration: 1000. Loss: 0.2941732108592987. Accuracy: 93.79%\n",
      "Iteration: 1500. Loss: 0.14714999496936798. Accuracy: 95.05%\n",
      "Iteration: 2000. Loss: 0.1753612905740738. Accuracy: 95.51%\n",
      "Iteration: 2500. Loss: 0.040193963795900345. Accuracy: 96.24%\n",
      "Iteration: 3000. Loss: 0.07396182417869568. Accuracy: 96.74%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3 (readout): 100 --> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {:.2f}%'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch weitere Beispiele...\n",
    "\n",
    "- **CNN** in PyTorch: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/\n",
    "\n",
    "- **RNN** in PyTorch: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/\n",
    "\n",
    "- **LSTM** in PyTorch: https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen\n",
    "\n",
    "**Deep Learning Wizard, \"Practical PyTorch\"**: https://www.deeplearningwizard.com/deep_learning/intro/ (**:-D !!!**)\n",
    "\n",
    "**Udacity**:\n",
    "[Deep Learning with PyTorch (Tutorial)](https://www.udacity.com/course/deep-learning-pytorch--ud188), \n",
    "[Deep Learning with PyTorch (Repo)](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch)\n",
    "\n",
    "\n",
    "**PyTorch, Neural Networks - PyTorch Tutorials**: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "**PyTorch Documentation**: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "\n",
    "Siraj Raval, \"Pytorch in 5 Minutes\": https://www.youtube.com/watch?v=nbJ-2G2GXL0 (very quick start)\n",
    "\n",
    "IAML (Italian Association for Machine Learning):\n",
    "[\"Fun with PyTorch - Part 1: Variables and Gradients\"](https://iaml.it/blog/fun-with-pytorch-part-1)\n",
    "\n",
    "\n",
    "\n",
    "Adventures in Machine Learning, \"A PyTorch tutorial – deep learning in Python\", http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/\n",
    "\n",
    "Ayoosh Kathuria, \"Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works\" (Autograd): https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
