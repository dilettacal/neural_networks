{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://maxpowerwastaken.github.io/blog/exploring-the-mnist-digits-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports at the beginning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata       \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets.base import get_data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, explore and prepare dataset\n",
    "\n",
    "The MNIST dataset is a classic Machine Learning dataset you can get it and more information about it from the website of [Yann Lecun](http://yann.lecun.com/exdb/mnist/). MNIST contains handwrittin digits and is split into a tranings set of 60000 examples and a test set of 10000 examples. You can use the module ```sklearn``` to load the MNIST dataset in a convenient way. \n",
    "easy load, mldata.org, orginal mnist, mnist link and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits (70000, 784)\n",
      "labels (70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original') #load MNIST\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] #separate images and labels\n",
    "\n",
    "# shape of MNIST data\n",
    "print('digits', X.shape)\n",
    "print('labels',y.shape)\n",
    "\n",
    "#datasets from sklearn:\n",
    "#print(get_data_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  64 253 255  63   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  96 205 251 253 205 111\n",
      "   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  96 189 251 251 253 251 251  31   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  16  64 223 244 251 251 211 213\n",
      " 251 251  31   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  80 181 251 253 251 251 251  94  96 251 251  31   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  92 253 253 253 255 253 253 253\n",
      "  95  96 253 253  31   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  92 236 251 243 220 233 251 251 243  82  96 251 251  31   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  80 253 251 251 188   0  96 251\n",
      " 251 109   0  96 251 251  31   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  96 240 253 243 188  42   0  96 204 109   4   0  12 197 251  31   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 221 251 253 121   0   0   0\n",
      "  36  23   0   0   0   0 190 251  31   0   0   0   0   0   0   0   0   0\n",
      "   0   0  48 234 253   0   0   0   0   0   0   0   0   0   0   0 191 253\n",
      "  31   0   0   0   0   0   0   0   0   0   0  44 221 251 251   0   0   0\n",
      "   0   0   0   0   0   0   0  12 197 251  31   0   0   0   0   0   0   0\n",
      "   0   0   0 190 251 251 251   0   0   0   0   0   0   0   0   0   0  96\n",
      " 251 251  31   0   0   0   0   0   0   0   0   0   0 190 251 251 113   0\n",
      "   0   0   0   0   0   0   0   0  40 234 251 219  23   0   0   0   0   0\n",
      "   0   0   0   0   0 190 251 251  94   0   0   0   0   0   0   0   0  40\n",
      " 217 253 231  47   0   0   0   0   0   0   0   0   0   0   0 191 253 253\n",
      " 253   0   0   0   0   0   0  12 174 253 253 219  39   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  67 236 251 251 191 190 111  72 190 191 197\n",
      " 251 243 121  39   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  63 236 251 253 251 251 251 251 253 251 188  94   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  27 129 253 251 251 251 251\n",
      " 229 168  15   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  95 212 251 211  94  59   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#How does an image look like?\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visualization of MNIST we will plot a digit. Each line represents an image in flatten form (all pixel in a row). We have change the shape from a vector back to a matrix of the original shape to plot the image. In the case of MNIST this means a conversion of 784 pixel into 28x28 pixel. In addition we will check the label of that digit to verify it correspond to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9512\n",
      "label: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_mnist_digit(digit):\n",
    "    image = digit.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='binary', interpolation='bicubic')\n",
    "\n",
    "#choose a random number, plot it and check label \n",
    "random_number = np.random.randint(1,60001)\n",
    "print(random_number)\n",
    "print('label:',y[random_number]) \n",
    "plot_mnist_digit(X[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a glimpse into MNIST let us explore it a bit further. Write a function ``` plot_mnist_digits(data, examples_each_row)``` that plots configurable number of examples for each class, like:\n",
    "![MNIST Examples](images/MNIST_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digits(data, examples_each_row):\n",
    "    ############################################\n",
    "    #TODO: Write a function that plots as many #    \n",
    "    #      examples of each class as defiend   #\n",
    "    #      by 'examples_each_row'              #\n",
    "    ############################################\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "plot_mnist_digits(X, examples_each_row=11)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring MNIST let us prepare the date for our linear classifier. First we need to separate traning and test data. Further we will shuffle the traning data to get a random distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a linear classifier using Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model to classify the MNIST dataset with the following equation:\n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_{i=1}^{M} -log\\; \\left ( \\frac{e^{h(x_j,\\Theta)}}{\\sum_{k=1}^{K}e^{h(x_k,\\Theta)}} \\right)_i + \\frac{\\lambda}{2} \\sum_{}^{} \\Theta^2, \\: with \\;\\; h(X,\\Theta) = X * \\Theta $$\n",
    "\n",
    "Using the universal equation for a loss function we can see the separate parts of that hugh equation.  \n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "We will implement each part on its own and put them together. That way it is much easier to understand whats going on. Let us start with the score function or hypothesis:\n",
    "\n",
    "$$h(X,\\Theta) = X * \\Theta$$\n",
    "\n",
    "It is possible to calculate all score values with one matrix multiplication ([dot product](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dot.html)) so we can use the whole traning data $X$ instead of one digit $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_scores(X,theta):\n",
    "    return np.dot(X,theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19882.99858204, 18181.69866387, 18931.3253929 , 18173.17776982,\n",
       "       19578.89813841, 19032.87217456, 18783.81172563, 20644.54114728,\n",
       "       19197.81089049, 20936.64248217])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Thetas should have dimension KxD\n",
    "theta = np.random.rand(X_train.shape[1],len(np.unique(y_train)))\n",
    "class_scores(X_train, theta)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we can define the data loss funtion $L_i$. We assume the score values are unnormalized log probabilities and we use the softmax function to calculate probabilities.\n",
    "$$ P(Y=j\\mid X=x_i) = \\frac{e^{s_j}}{\\sum_{k=1}^{K}e^{s_k}} $$\n",
    "$$ L_i = -log\\;P(Y=j\\mid X=x_i) $$\n",
    "\n",
    "Hint: If the correct classes (labels) are in a [one hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) shape you can use a matrix multiplication to extract the correct class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support function to convert label vector into a one hot encoding matrix\n",
    "def onehot_encode_label(label):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    label = label.reshape(len(label), 1)\n",
    "    onehot_encoded_label = onehot_encoder.fit_transform(label)\n",
    "    return onehot_encoded_label\n",
    "\n",
    "# Calculate class probability distribution for each digit from given class scores\n",
    "def softmax(class_scores):\n",
    "    ############################################\n",
    "    #TODO: Use the softmax function to compute #\n",
    "    #      class probabilties                  #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "\n",
    "# Compute data_loss L_i for the correct class\n",
    "def data_loss(class_probabilities, onehot_encode_label):\n",
    "    ############################################\n",
    "    #TODO: With hot encoded labels and class   #\n",
    "    #      probabilties calculate data loss    #\n",
    "    #      L_i                                 #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate loss $L$ using the defined functions. \n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "Besides the loss L we will have to calculate the gradient for our loss function $L$. To minimize our loss we will need the gradient. For more information about the gradient you can use additional sources, like that good [blog post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, theta, lam):\n",
    "    encoded_labels = onehot_encode_label(y)           # also needed for the gradient, therefore separated calculated\n",
    "    probabilities = softmax(class_scores(X,theta))    # also needed for the gradient, therefore separated calculated\n",
    "    loss_Li = data_loss(probabilities,encoded_labels) \n",
    "    \n",
    "    m = X.shape[0]                                    # number of training data for normalization\n",
    "    l2_regularization = (lam/2)*np.sum(theta*theta)   # regularization loss\n",
    "  \n",
    "    ############################################\n",
    "    #TODO: Put everthing together and calculte #\n",
    "    #      loss L and gradient dL with given   #\n",
    "    #      variables.                          #\n",
    "    ############################################\n",
    " \n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the cost using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(traning_data, traning_label, theta, lam=0.5, iterations=100, learning_rate=1e-5):\n",
    "    losses = []\n",
    "    ############################################\n",
    "    #TODO: Optimize loss with gradient descent #\n",
    "    #      update rule. Return a final model   #\n",
    "    #      and a history of loss values.       #\n",
    "    ############################################\n",
    "    \n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################    \n",
    "    return theta, losses\n",
    "\n",
    "# Initialize learnable parameters theta \n",
    "theta = np.zeros([X_train.shape[1],len(np.unique(y_train))])\n",
    "# Start optimization with traning data, theta and optional hyperparameters\n",
    "opt_model, loss_history = gradient_descent(X_train,y_train,theta,iterations=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "Let us look at the optimization results. Final loss tells us how far we could reduce costs during traning process. Further we can use the first loss value as a sanity check and validate our implementation of the loss function works as intended. Recall loss value after first iteration should be $ log\\:c$ with $c$ being number of classes. To visulize the whole tranings process we can plot losss values from each iteration as a loss curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7ed565fad50c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check loss after last iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'last iteration loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Sanity check: first loss should be ln(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first iteration loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot a loss curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# check loss after last iteration\n",
    "print('last iteration loss:',loss_history[-1])\n",
    "# Sanity check: first loss should be ln(10)\n",
    "print('first iteration loss:',loss_history[0])\n",
    "# Plot a loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation above gave us some inside about the optimization process but did not quantified our final model. One possibility is to calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelAccuracy(X,y,theta):\n",
    "    # calculate probabilities for each digit\n",
    "    probabilities = softmax(np.dot(X,theta))\n",
    "    # class with highest probability will be predicted\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    # Sum all correct predictions and divied by number of data\n",
    "    accuracy = (sum(prediction == y))/X.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "print('Training accuracy: ', modelAccuracy(X_train,y_train,opt_model))\n",
    "print('Test accuracy: ', modelAccuracy(X_test,y_test,opt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that quantification is limited. A more gerenell approach is to calculate a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and get different model measurements from it. A good overview for model measurements is provided by the wikipedia article of [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). We implement a confusion matrix for our model and calculate a [F1 score](https://en.wikipedia.org/wiki/F1_score) and ```print()``` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(X,y,theta):\n",
    "    ############################################\n",
    "    #TODO: Calculate a confusion matrix for    # \n",
    "    #      and it.                             #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "def f1Score(confMatrix):\n",
    "    ############################################\n",
    "    #TODO: Calculate a F1 score from a given   #\n",
    "    #      confusion matrix.                   #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting is to plot a part of $theta$, because you can visualize the learned templates for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(opt_model[:,0],[28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
